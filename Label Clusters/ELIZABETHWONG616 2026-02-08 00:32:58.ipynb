{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zqarj6xbhng2kkkdf6km",
   "authorId": "6942764638652",
   "authorName": "ELIZABETHWONG616",
   "authorEmail": "wo_eliza@live.concordia.ca",
   "sessionId": "f6e671ae-5712-4349-bcd7-22e2c26040b8",
   "lastEditTime": 1770534621362
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "e603298a-a7e1-41ee-b5ab-223748054bad",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def generate_cluster_labels(session):\n    \"\"\"Generate descriptive labels for clusters based on caption content.\"\"\"\n    labels = {}\n    \n    # Get sample captions for each cluster\n    cluster_content = session.sql(\"\"\"\n        SELECT \n            c.CLUSTER_ID,\n            ARRAY_AGG(s.CAPTION) WITHIN GROUP (ORDER BY RANDOM()) AS CAPTIONS\n        FROM MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA s\n        JOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_EMBEDS e \n            ON s.CAPTION = e.CONTENT\n        JOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_CLUSTERS c \n            ON e.CONTENT_HASH = c.CONTENT_HASH\n        GROUP BY c.CLUSTER_ID\n    \"\"\").to_pandas()\n    \n    for _, row in cluster_content.iterrows():\n        cluster_id = row['CLUSTER_ID']\n        captions = row['CAPTIONS']\n        \n        # Take up to 10 sample captions\n        sample_captions = captions[:10] if len(captions) > 10 else captions\n        captions_text = \"\\n\".join([f\"- {c[:200]}\" for c in sample_captions])\n        \n        prompt = f\"\"\"Analyze these social media captions from a content cluster and generate a short descriptive label (2-4 words) that captures the main theme or topic:\n\n{captions_text}\n\nReturn only the label, nothing else.\"\"\"\n        \n        result = session.sql(f\"\"\"\n            SELECT SNOWFLAKE.CORTEX.COMPLETE('gemini-3-pro', $${prompt}$$) AS label\n        \"\"\").collect()\n        \n        labels[cluster_id] = result[0]['LABEL'].strip()\n    \n    return labels",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9e71e450-aa60-4ea8-a997-f21ed289f66c",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# First, get your clustered data with numeric features\ncluster_df = session.sql(\"\"\"\n    SELECT \n        c.CLUSTER_ID,\n        s.LIKESCOUNT,\n        s.COMMENTSCOUNT,\n        s.LIKESCOUNT + s.COMMENTSCOUNT AS TOTAL_INTERACTIONS\n    FROM MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA s\n    JOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_EMBEDS e \n        ON s.CAPTION = e.CONTENT\n    JOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_CLUSTERS c \n        ON e.CONTENT_HASH = c.CONTENT_HASH\n\"\"\").to_pandas()\n\n# Generate labels using numeric features\ncluster_labels = generate_cluster_labels(session)\n\n# After generating cluster_labels, create a DataFrame and save to Snowflake\nlabels_df = pd.DataFrame([\n    {'CLUSTER_ID': k, 'CLUSTER_LABEL': v} \n    for k, v in cluster_labels.items()\n])\n\n# Write to Snowflake table\nsession.write_pandas(\n    labels_df, \n    'CLUSTER_LABELS',\n    database='MCWICS_HACKATHON_DATA',\n    schema='PUBLIC',\n    auto_create_table=True,\n    overwrite=True\n)\n\nprint(\"Cluster labels saved to MCWICS_HACKATHON_DATA.PUBLIC.CLUSTER_LABELS\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4591a009-777e-4f53-a503-d797b24ce532",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "-- Cell 1: Prepare data by joining with clusters\n-- Get interaction data with cluster assignments\nSELECT \n    s.TIMESTAMP,\n    s.LIKESCOUNT,\n    s.COMMENTSCOUNT,\n    s.LIKESCOUNT + s.COMMENTSCOUNT AS TOTAL_INTERACTIONS,\n    c.CLUSTER_ID\nFROM MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA s\nJOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_EMBEDS e \n    ON s.CAPTION = e.CONTENT\nJOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_CLUSTERS c \n    ON e.CONTENT_HASH = c.CONTENT_HASH\nJOIN MCWICS_HACKATHON_DATA.PUBLIC.CLUSTER_LABELS l\n    ON l.cluster_id = c.cluster_id\nORDER BY c.CLUSTER_ID, s.TIMESTAMP",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f259e4c3-6ff9-4ed9-99d4-edb6751131d2",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Aggregate to daily level per cluster\nagg_query = \"\"\"\nSELECT \n    DATE_TRUNC('DAY', s.TIMESTAMP) AS DATE,\n    c.CLUSTER_ID,\n    SUM(s.LIKESCOUNT + s.COMMENTSCOUNT) AS TOTAL_INTERACTIONS\nFROM MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA s\nJOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_EMBEDS e \n    ON s.CAPTION = e.CONTENT\nJOIN MCWICS_HACKATHON_DATA.PUBLIC.SCRAPEDATA_CAPTION_CLUSTERS c \n    ON e.CONTENT_HASH = c.CONTENT_HASH\nGROUP BY DATE_TRUNC('DAY', s.TIMESTAMP), c.CLUSTER_ID\nORDER BY c.CLUSTER_ID, DATE\n\"\"\"\n\nsession.sql(\"CREATE OR REPLACE VIEW forecast_input AS \" + agg_query).collect()\n\n# Add ON_ERROR => 'SKIP' to skip clusters with insufficient data\nsession.sql(\"\"\"\nCREATE OR REPLACE SNOWFLAKE.ML.FORECAST interaction_forecast_model(\n    INPUT_DATA => TABLE(forecast_input),\n    SERIES_COLNAME => 'CLUSTER_ID',\n    TIMESTAMP_COLNAME => 'DATE',\n    TARGET_COLNAME => 'TOTAL_INTERACTIONS',\n    CONFIG_OBJECT => {'ON_ERROR': 'SKIP'}\n)\n\"\"\").collect()\n\nprint(\"Forecast model trained successfully!\")\n\n# Check which series failed\nsession.sql(\"CALL interaction_forecast_model!SHOW_TRAINING_LOGS()\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9086330-332b-4e5a-a0b5-7973e07a87a6",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Analyze forecast results and generate LLM insights with cluster labels\nimport json\nfrom datetime import datetime\n\n# Get forecast results\nforecast_results = session.sql(\"\"\"\nSELECT * FROM TABLE(interaction_forecast_model!FORECAST(FORECASTING_PERIODS => 7))\nORDER BY SERIES, TS\n\"\"\").to_pandas()\n\n# Get forecast summary stats per cluster\nforecast_summary = forecast_results.groupby('SERIES').agg({\n    'FORECAST': ['mean', 'sum', 'max'],\n    'UPPER_BOUND': 'max',\n    'LOWER_BOUND': 'min'\n}).round(2)\n\nforecast_summary.columns = ['avg_forecast', 'total_forecast', 'peak_forecast', 'upper_bound', 'lower_bound']\nforecast_summary = forecast_summary.reset_index()\n\n# Get cluster labels\nlabels_df = session.sql(\"\"\"\n    SELECT CLUSTER_ID, CLUSTER_LABEL \n    FROM MCWICS_HACKATHON_DATA.PUBLIC.CLUSTER_LABELS\n\"\"\").to_pandas()\n\n# Convert both columns to same type before merge\nforecast_summary['SERIES'] = forecast_summary['SERIES'].astype(int)\nlabels_df['CLUSTER_ID'] = labels_df['CLUSTER_ID'].astype(int)\n\nforecast_summary = forecast_summary.merge(labels_df, left_on='SERIES', right_on='CLUSTER_ID', how='left')\nforecast_summary['CLUSTER_LABEL'] = forecast_summary['CLUSTER_LABEL'].fillna(forecast_summary['SERIES'].apply(lambda x: f\"Cluster {x}\"))\n\n# Build context for LLM with cluster labels\nsummary_text = \"Forecasted engagement by content cluster (next 7 days):\\n\\n\"\nfor _, row in forecast_summary.iterrows():\n    summary_text += f\"- {row['CLUSTER_LABEL']} (ID {row['SERIES']}): avg={row['avg_forecast']:.0f}, total={row['total_forecast']:.0f}, peak={row['peak_forecast']:.0f}\\n\"\n\nprompt = f\"\"\"You are a social media strategist. Analyze this engagement forecast data and provide actionable insights.\n\nThe clusters represent different types of social media content based on their captions and engagement patterns:\n\n{summary_text}\n\nAnswer these questions:\n1. Which content type shows the highest predicted engagement? Reference by its label name.\n2. Based on the cluster labels, what themes or content strategies are working best?\n3. What specific recommendations would you give to maximize community engagement?\n4. Which content types should be deprioritized and why?\n\nReference cluster labels by name, and provide actionable recommendations.\nBe short and concise so that the reader can understand the insights quickly.\"\"\"\n\n# Generate insights using LLM\ninsights_result = session.sql(f\"\"\"\n    SELECT SNOWFLAKE.CORTEX.COMPLETE('gemini-3-pro', $${prompt}$$) AS insights\n\"\"\").collect()\n\ninsights = insights_result[0]['INSIGHTS']\n\n# Save insights to table\ninsights_df = pd.DataFrame([{\n    'GENERATED_AT': datetime.now(),\n    'INSIGHTS': insights,\n    'FORECAST_SUMMARY': summary_text,\n    'NUM_CLUSTERS': len(forecast_summary)\n}])\n\nsession.write_pandas(\n    insights_df,\n    'ENGAGEMENT_INSIGHTS',\n    database='MCWICS_HACKATHON_DATA',\n    schema='PUBLIC',\n    auto_create_table=True,\n    overwrite=True\n)\n\nprint(\"Insights saved to MCWICS_HACKATHON_DATA.PUBLIC.ENGAGEMENT_INSIGHTS\")\n\n# Display results\nst.subheader(\"AI-Generated Engagement Insights\")\nst.markdown(insights)\n\n# Show summary table with labels\nst.subheader(\"Forecast Summary by Content Type\")\ndisplay_cols = ['CLUSTER_LABEL', 'avg_forecast', 'total_forecast', 'peak_forecast']\nst.dataframe(forecast_summary[display_cols].sort_values('total_forecast', ascending=False))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c57ed781-c237-4c0f-b6b4-b9e02e00ab6a",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}