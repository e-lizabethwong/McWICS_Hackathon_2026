{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "al56hq2hhrqkfpqbd54k",
   "authorId": "6942764638652",
   "authorName": "ELIZABETHWONG616",
   "authorEmail": "wo_eliza@live.concordia.ca",
   "sessionId": "71c8ffa4-3338-446e-ba1d-9c3cc5885972",
   "lastEditTime": 1770534215437
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "2aed2b1f-9fe6-47aa-af98-d88364e2b44b",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# ---- Parameters youâ€™ll tune ----\nSOURCE_TABLE = \"SCRAPEDATA\"\nTEXT_COL = \"CAPTION\"\n\n# Output tables\nEMBED_TABLE = \"SCRAPEDATA_CAPTION_EMBEDS\"\nCLUSTER_TABLE = \"SCRAPEDATA_CAPTION_CLUSTERS\"\n\n# Model + clustering hyperparams\nEMBED_MODEL = \"snowflake-arctic-embed-m\"   # also commonly: \"e5-base-v2\" :contentReference[oaicite:1]{index=1}\nK = 12                                     # start here; tune later\nRANDOM_STATE = 42\n\n# For large tables, start with a sample for fitting, then score all rows\nFIT_SAMPLE_N = 50   # increase if you can (depends on warehouse & budget)\nBATCH_SIZE = 200     # batch scoring/writeback\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f991e8b-92d3-4628-895c-485ab89e42e4",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col, sql_expr\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import MiniBatchKMeans\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8209215e-7438-4a8a-8263-dc39b070df3b",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Create embeddings table if it doesn't exist yet\n\ncreate_embeds_sql = f\"\"\"\nCREATE TABLE IF NOT EXISTS {EMBED_TABLE} AS\nSELECT\n  /* If you have a unique ID column, include it here instead of HASH */\n  SHA2(TO_VARCHAR({TEXT_COL}), 256) AS content_hash,\n  {TEXT_COL} AS content,\n  AI_EMBED('{EMBED_MODEL}', {TEXT_COL}) AS embedding,\n  CURRENT_TIMESTAMP() AS embedded_at,\n  '{EMBED_MODEL}' AS embedding_model\nFROM {SOURCE_TABLE}\nWHERE {TEXT_COL} IS NOT NULL\n  AND LENGTH(TRIM({TEXT_COL})) > 0\n;\n\"\"\"\nsession.sql(create_embeds_sql).collect()\n\n# # If table already existed and you want to (re)embed new rows only:\n# insert_new_sql = f\"\"\"\n# INSERT INTO {EMBED_TABLE} (content_hash, content, embedding, embedded_at, embedding_model)\n# SELECT\n#   SHA2(TO_VARCHAR(s.{TEXT_COL}), 256) AS content_hash,\n#   s.{TEXT_COL} AS content,\n#   AI_EMBED('{EMBED_MODEL}', s.{TEXT_COL}) AS embedding,\n#   CURRENT_TIMESTAMP() AS embedded_at,\n#   '{EMBED_MODEL}' AS embedding_model\n# FROM {SOURCE_TABLE} s\n# LEFT JOIN {EMBED_TABLE} e\n#   ON e.content_hash = SHA2(TO_VARCHAR(s.{TEXT_COL}), 256)\n# WHERE s.{TEXT_COL} IS NOT NULL\n#   AND LENGTH(TRIM(s.{TEXT_COL})) > 0\n#   AND e.content_hash IS NULL\n# ;\n# \"\"\"\n# session.sql(insert_new_sql).collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5843922f-e0f7-4088-a0fb-e5692e03861c",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "df_embed = session.table(EMBED_TABLE).select(\"content_hash\", \"embedding\")\n\n# Sample for training\ndf_fit = df_embed.sample(n=FIT_SAMPLE_N) if FIT_SAMPLE_N else df_embed\n\npdf_fit = df_fit.to_pandas()\npdf_fit.head()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9306da5-d751-4520-bb9f-46a67f9e1706",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "def to_vec(x):\n    # x is usually list-like already; this keeps it robust\n    if x is None:\n        return None\n    return np.asarray(x, dtype=np.float32)\n\nX = np.vstack([to_vec(v) for v in pdf_fit[\"EMBEDDING\"].tolist() if v is not None])\nhashes_fit = pdf_fit[\"CONTENT_HASH\"].tolist()\n\nX.shape\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65caf555-c623-437a-9823-bce1891f2375",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "kmeans = MiniBatchKMeans(\n    n_clusters=K,\n    random_state=RANDOM_STATE,\n    batch_size=2048,\n    n_init=\"auto\"\n)\nkmeans.fit(X)\n\n# Optional quick sanity check: cluster sizes on the fit sample\nlabels_fit = kmeans.predict(X)\n(pd.Series(labels_fit).value_counts().sort_index())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7388979a-c03a-4d95-b90c-dff0c265b9f6",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS {CLUSTER_TABLE} (\n  content_hash STRING,\n  cluster_id INTEGER,\n  k INTEGER,\n  embedding_model STRING,\n  scored_at TIMESTAMP_NTZ\n);\n\"\"\").collect()\n\n# # Clear previous run for same (K, model) if you want a clean rerun:\n# session.sql(f\"\"\"\n# DELETE FROM {CLUSTER_TABLE}\n# WHERE k = {K}\n#   AND embedding_model = '{EMBED_MODEL}';\n# \"\"\").collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "247012ab-6a37-46cf-af50-334aea255a64",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "total = session.table(EMBED_TABLE).count()\ntotal\noffset = 0\nwhile offset < total:\n    batch_df = (\n        session.table(EMBED_TABLE)\n        .select(\"content_hash\", \"embedding\")\n        .sort(col(\"content_hash\"))\n        .limit(BATCH_SIZE, offset=offset)\n    )\n\n    batch_pdf = batch_df.to_pandas()\n    if batch_pdf.empty:\n        break\n\n    Xb = np.vstack([to_vec(v) for v in batch_pdf[\"EMBEDDING\"].tolist() if v is not None])\n    hb = batch_pdf[\"CONTENT_HASH\"].tolist()\n\n    preds = kmeans.predict(Xb)\n\n    out_pdf = pd.DataFrame({\n        \"CONTENT_HASH\": hb,\n        \"CLUSTER_ID\": preds.astype(int),\n        \"K\": K,\n        \"EMBEDDING_MODEL\": EMBED_MODEL\n    })\n\n    out_sdf = session.create_dataframe(out_pdf)\n    out_sdf = out_sdf.with_column(\"SCORED_AT\", sql_expr(\"CURRENT_TIMESTAMP()\"))\n\n    out_sdf.write.mode(\"append\").save_as_table(CLUSTER_TABLE)\n\n    offset += BATCH_SIZE\n\nsession.table(CLUSTER_TABLE).filter((col(\"K\")==K) & (col(\"EMBEDDING_MODEL\")==EMBED_MODEL)).count()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91edd263-1962-4cdf-9e62-e00497ca02ca",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# Join back to content and show a few examples per cluster\njoined = session.table(CLUSTER_TABLE).alias(\"c\").join(\n    session.table(EMBED_TABLE).select(\"content_hash\", \"content\").alias(\"e\"),\n    col(\"content_hashc\") == col(\"content_hashe\"),\n    how=\"inner\"\n).select(\n    col(\"cluster_id\"),\n    col(\"content\")\n).filter(col(\"cluster_id\").is_not_null())\n\n# Pull a small sample for exploration in the notebook UI\npdf_examples = joined.sample(n=2000).to_pandas()\npdf_examples.groupby(\"CLUSTER_ID\").head(5)\n",
   "execution_count": null
  }
 ]
}